{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ultra-clean A/B pipeline: prep -> run (subprocess) -> collect -> plot\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, sys, re, json, time, subprocess, textwrap\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# Project paths\n",
    "# -----------------------\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent  # adjust if needed\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"fuzzy\"\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results\" / \"fuzzy\"\n",
    "for p in (DATA_DIR, RESULTS_DIR): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Step 1: Prep (URL + SF)\n",
    "# -----------------------\n",
    "def _filename_from_url(url: str) -> str:\n",
    "    return Path(url.split(\"?\")[0]).name\n",
    "\n",
    "def _download(url: str, base_dir: Path) -> Path:\n",
    "    filename = _filename_from_url(url)\n",
    "    name_root = Path(filename).stem\n",
    "    dst_dir = base_dir / name_root\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out = dst_dir / filename\n",
    "    if out.exists():\n",
    "        print(f\"[download] Using cached: {out}\")\n",
    "        return out\n",
    "    legacy = base_dir / filename\n",
    "    if legacy.exists():\n",
    "        out.write_bytes(legacy.read_bytes())\n",
    "        print(f\"[download] Moved legacy file -> {out}\")\n",
    "        return out\n",
    "    print(f\"[download] Fetch {url}\")\n",
    "    r = requests.get(url, timeout=60); r.raise_for_status()\n",
    "    out.write_bytes(r.content)\n",
    "    print(f\"[download] Saved {out}\")\n",
    "    return out\n",
    "\n",
    "def prepare_dataset(url: str, sf: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        'dataset_name', 'original',\n",
    "        'floating_text', 'floating_parquet',\n",
    "        'fixed_parquet', 'quant_mult'\n",
    "      }\n",
    "    Uses your repo scripts:\n",
    "      - scripts.replicate_file.replicate_file\n",
    "      - scripts.fixedpoint_normalize.normalize_file\n",
    "    \"\"\"\n",
    "    # Ensure project on PYTHONPATH for *this* process\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "    original = _download(url, DATA_DIR)\n",
    "    dataset_name = original.parent.name\n",
    "\n",
    "    # 1) replicate floating text (SF-concatenated)\n",
    "    from scripts.replicate_file import replicate_file\n",
    "    sf = max(1, int(sf))\n",
    "    floating_text = original.with_name(f\"{original.stem}_SF{sf}_floating{original.suffix}\")\n",
    "    if floating_text.exists():\n",
    "        print(f\"[prep] Using existing: {floating_text.name}\")\n",
    "    else:\n",
    "        replicate_file(str(original), sf, str(floating_text))\n",
    "        print(f\"[prep] Made: {floating_text.name}\")\n",
    "\n",
    "    # 2) make fixed parquet + quant_mult AND floating parquet\n",
    "    from scripts.fixedpoint_normalize import normalize_file\n",
    "    stem = floating_text.stem.replace(\"_floating\", \"\")\n",
    "    fixed_parquet   = floating_text.with_name(f\"{stem}_fixed.parquet\")\n",
    "    quant_file      = floating_text.with_name(f\"{stem}_quant_mult.txt\")\n",
    "    floating_parquet= floating_text.with_suffix(\".parquet\")\n",
    "\n",
    "    if not fixed_parquet.exists() or not quant_file.exists() or not floating_parquet.exists():\n",
    "        print(\"[prep] Running normalize_file to get parquet(s) & quant_mult …\")\n",
    "        # write_fixed_text=False keeps things parquet-only for speed\n",
    "        _ = normalize_file(str(floating_text), write_fixed_text=False)\n",
    "\n",
    "    if not fixed_parquet.exists():    raise FileNotFoundError(f\"Missing: {fixed_parquet}\")\n",
    "    if not floating_parquet.exists(): raise FileNotFoundError(f\"Missing: {floating_parquet}\")\n",
    "    if not quant_file.exists():       raise FileNotFoundError(f\"Missing: {quant_file}\")\n",
    "\n",
    "    quant_mult = int(quant_file.read_text().strip())\n",
    "    return {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"original\": str(original),\n",
    "        \"floating_text\": str(floating_text),\n",
    "        \"floating_parquet\": str(floating_parquet),\n",
    "        \"fixed_parquet\": str(fixed_parquet),\n",
    "        \"quant_mult\": quant_mult,\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Step 2: Run miners in separate processes (clean)\n",
    "# -------------------------------------------------\n",
    "def _run_subprocess(args: List[str], log_path: Path, err_path: Path, cwd: Path, extra_env: Optional[Dict[str, str]]=None) -> int:\n",
    "    env = os.environ.copy()\n",
    "    # Ensure the child sees your repo\n",
    "    env[\"PYTHONPATH\"] = str(PROJECT_ROOT) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
    "    if extra_env:\n",
    "        env.update({k: str(v) for k, v in extra_env.items()})\n",
    "    print(\"[run] \", \" \".join(args))\n",
    "    with open(log_path, \"w\") as out, open(err_path, \"w\") as err:\n",
    "        proc = subprocess.run(args, cwd=cwd, env=env, stdout=out, stderr=err, text=True)\n",
    "    print(f\"[run] exit={proc.returncode}  log={log_path.name}  err={err_path.name}\")\n",
    "    return proc.returncode\n",
    "\n",
    "def run_cuffi_cli(fixed_parquet: str, quant_mult: int, sup_int: int, out_dir: Path,\n",
    "                  allocator=\"rmm_managed\", gds=\"off\", pinned=True, managed_prefetch=True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    cuFFIMiner expects pre-scaled parquet (item, prob:uint32, txn_id).\n",
    "    NOTE (your toggle): gds='off' => force cuFile => GDS ON. gds='on' => POSIX => GDS OFF.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    patterns_path = out_dir / f\"patterns_cuffi_{allocator}_{gds}_{'pin' if pinned else 'nopin'}_sup{sup_int}.txt\"\n",
    "    log_path      = out_dir / f\"cuffi_sup{sup_int}.out\"\n",
    "    err_path      = out_dir / f\"cuffi_sup{sup_int}.err\"\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"src.algorithms.fuzzy.cuFFIMiner\",\n",
    "        str(fixed_parquet),\n",
    "        str(int(sup_int)),\n",
    "        str(int(quant_mult)),\n",
    "        \"-o\", str(patterns_path),\n",
    "        \"--allocator\", allocator,\n",
    "        \"--gds\", gds,\n",
    "    ]\n",
    "    if pinned:           cmd.append(\"--pinned\")\n",
    "    if managed_prefetch: cmd.append(\"--managed-prefetch\")\n",
    "\n",
    "    rc = _run_subprocess(cmd, log_path, err_path, cwd=PROJECT_ROOT)\n",
    "    return {\"rc\": rc, \"patterns\": str(patterns_path), \"stdout\": str(log_path), \"stderr\": str(err_path)}\n",
    "\n",
    "def run_naive_cli(floating_text_or_parquet: str, quant_mult: int, sup_int: int, out_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    naiveFFIMiner enforces: KVIKIO_COMPAT_MODE=ON (POSIX path; GDS OFF) + device-only pool.\n",
    "    It expects float min_support and accepts --quant-mult to lock scaling.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sup_float = sup_int / max(1, int(quant_mult))\n",
    "    patterns_path = out_dir / f\"patterns_naive_sup{sup_int}.txt\"\n",
    "    log_path      = out_dir / f\"naive_sup{sup_int}.out\"\n",
    "    err_path      = out_dir / f\"naive_sup{sup_int}.err\"\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"src.algorithms.fuzzy.naiveFFIMiner\",\n",
    "        str(floating_text_or_parquet),\n",
    "        f\"{sup_float:.12g}\",\n",
    "        \"--quant-mult\", str(int(quant_mult)),\n",
    "        \"-o\", str(patterns_path),\n",
    "    ]\n",
    "    rc = _run_subprocess(cmd, log_path, err_path, cwd=PROJECT_ROOT)\n",
    "    return {\"rc\": rc, \"patterns\": str(patterns_path), \"stdout\": str(log_path), \"stderr\": str(err_path)}\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Parse logs -> metrics -> plots\n",
    "# ---------------------------------------\n",
    "_METRIC_PATTERNS = {\n",
    "    \"exec_time\":          re.compile(r\"Execution Time:\\s*([0-9.]+)\\s*seconds\", re.I),\n",
    "    \"cpu_mem_mb\":         re.compile(r\"Peak CPU Memory Usage:\\s*([0-9.]+)\\s*MB\", re.I),\n",
    "    \"gpu_mem_mb\":         re.compile(r\"Peak GPU \\(driver\\) Used:\\s*([0-9.]+)\\s*MB\", re.I),\n",
    "    \"pool_used_mb\":       re.compile(r\"Peak Pool Used:\\s*([0-9.]+)\\s*MB\", re.I),\n",
    "    \"pool_total_mb\":      re.compile(r\"Peak Pool Total:\\s*([0-9.]+)\\s*MB\", re.I),\n",
    "    \"rmm_peak_mb\":        re.compile(r\"RMM Statistics Peak:\\s*([0-9.]+)\\s*MB\", re.I),\n",
    "    \"patterns_found\":     re.compile(r\"Patterns Found:\\s*([0-9]+)\", re.I),\n",
    "}\n",
    "\n",
    "def parse_metrics_from_log(log_path: Path) -> Dict[str, Optional[float]]:\n",
    "    text = Path(log_path).read_text(errors=\"ignore\")\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    for k, rgx in _METRIC_PATTERNS.items():\n",
    "        m = rgx.search(text)\n",
    "        out[k] = float(m.group(1)) if m else None\n",
    "    return out\n",
    "\n",
    "def collect_results(dataset_name: str, sf: int, quant_mult: int, supports: List[int], out_dir: Path) -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for sup in supports:\n",
    "        cuffi_log = out_dir / f\"cuffi_sup{sup}.out\"\n",
    "        naive_log = out_dir / f\"naive_sup{sup}.out\"\n",
    "\n",
    "        if cuffi_log.exists():\n",
    "            m = parse_metrics_from_log(cuffi_log)\n",
    "            rows.append({\n",
    "                \"dataset\": dataset_name, \"sf\": sf, \"algorithm\": \"cuFFIMiner\",\n",
    "                \"support_quant_int\": sup, \"quant_mult\": quant_mult,\n",
    "                **m\n",
    "            })\n",
    "        if naive_log.exists():\n",
    "            m = parse_metrics_from_log(naive_log)\n",
    "            rows.append({\n",
    "                \"dataset\": dataset_name, \"sf\": sf, \"algorithm\": \"naiveFFIMiner\",\n",
    "                \"support_quant_int\": sup, \"quant_mult\": quant_mult,\n",
    "                **m\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    # If only gpu_mem_mb present, keep; if you also want bytes:\n",
    "    if \"gpu_mem_mb\" in df.columns and df[\"gpu_mem_mb\"].notna().any():\n",
    "        df[\"gpu_mem_bytes\"] = df[\"gpu_mem_mb\"] * (1024**2)\n",
    "    return df\n",
    "\n",
    "# ---- plotting (PDF, LaTeX-friendly) ----\n",
    "plt.rcParams.update({\n",
    "    \"pdf.fonttype\": 42, \"ps.fonttype\": 42, \"figure.dpi\": 150,\n",
    "    \"font.size\": 11, \"axes.titlesize\": 12, \"axes.labelsize\": 11, \"legend.fontsize\": 9,\n",
    "})\n",
    "\n",
    "_LABELS = {\n",
    "    \"exec_time\": \"Execution Time (s)\",\n",
    "    \"cpu_mem_mb\": \"Peak CPU Memory (MB)\",\n",
    "    \"gpu_mem_mb\": \"Peak GPU (driver) Used (MB)\",\n",
    "    \"patterns_found\": \"Patterns Found\",\n",
    "}\n",
    "\n",
    "def _plot_metric(df: pd.DataFrame, metric: str, out_dir: Path, dataset_name: str):\n",
    "    if metric not in df.columns:\n",
    "        print(f\"[plot] Skip missing metric: {metric}\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(figsize=(5.0, 3.0))\n",
    "    for algo, sub in df.groupby(\"algorithm\", sort=False):\n",
    "        sub = sub.sort_values(\"support_quant_int\")\n",
    "        ax.plot(sub[\"support_quant_int\"].values, sub[metric].values, marker=\"o\", label=algo)\n",
    "    ax.set_xlabel(\"Support Threshold (quantized int)\")\n",
    "    ax.set_ylabel(_LABELS.get(metric, metric))\n",
    "    ax.set_title(f\"{dataset_name} — {_LABELS.get(metric, metric)}\")\n",
    "    ax.grid(alpha=0.25, linestyle=\":\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf = out_dir / f\"{dataset_name}_{metric}.pdf\"\n",
    "    fig.savefig(pdf, format=\"pdf\"); plt.close(fig)\n",
    "    print(f\"[plot] wrote {pdf}\")\n",
    "\n",
    "def plot_all(metrics_df: pd.DataFrame, dataset_name: str, figs_dir: Path, metrics: Optional[List[str]]=None):\n",
    "    ms = metrics or [\"exec_time\", \"cpu_mem_mb\", \"gpu_mem_mb\", \"patterns_found\"]\n",
    "    for m in ms: _plot_metric(metrics_df, m, figs_dir, dataset_name)\n",
    "    print(\"[plot] done.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Orchestrator (one-liner for your runs)\n",
    "# ----------------------------------------\n",
    "def run_pipeline(\n",
    "    dataset_url: str,\n",
    "    sf: int,\n",
    "    supports_quant_int: List[int],\n",
    "    *,\n",
    "    # cuFFI toggles\n",
    "    cuffi_allocator: str = \"rmm_managed\",\n",
    "    cuffi_gds: str = \"off\",      # 'off' => cuFile => GDS ON ; 'on' => POSIX => GDS OFF\n",
    "    cuffi_pinned: bool = False,\n",
    "    cuffi_prefetch: bool = True,\n",
    "    force: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Prep artifacts (URL + SF)\n",
    "    2) Run cuFFIMiner (GDS+UVM) AND naiveFFIMiner (no-GDS + device-only) in subprocesses\n",
    "    3) Parse logs -> CSV -> plots\n",
    "    \"\"\"\n",
    "    prep = prepare_dataset(dataset_url, sf)\n",
    "    dataset = prep[\"dataset_name\"]; quant_mult = prep[\"quant_mult\"]\n",
    "    ds_dir = RESULTS_DIR / dataset / f\"SF{sf}\"\n",
    "    logs_dir = ds_dir / \"logs\"; logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Run all supports\n",
    "    for sup in supports_quant_int:\n",
    "        cuffi_out = logs_dir / f\"cuffi_sup{sup}.out\"\n",
    "        naive_out = logs_dir / f\"naive_sup{sup}.out\"\n",
    "\n",
    "        if (not cuffi_out.exists()) or force:\n",
    "            run_cuffi_cli(\n",
    "                fixed_parquet=prep[\"fixed_parquet\"],\n",
    "                quant_mult=quant_mult,\n",
    "                sup_int=sup,\n",
    "                out_dir=ds_dir,\n",
    "                allocator=cuffi_allocator,\n",
    "                gds=cuffi_gds,\n",
    "                pinned=cuffi_pinned,\n",
    "                managed_prefetch=cuffi_prefetch,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[skip] cuFFI sup={sup} (log exists, use force=True to re-run)\")\n",
    "\n",
    "        if (not naive_out.exists()) or force:\n",
    "            run_naive_cli(\n",
    "                floating_text_or_parquet=prep[\"floating_parquet\"],\n",
    "                quant_mult=quant_mult,\n",
    "                sup_int=sup,\n",
    "                out_dir=ds_dir,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[skip] naive sup={sup} (log exists, use force=True to re-run)\")\n",
    "\n",
    "    # Collect -> CSV\n",
    "    df = collect_results(dataset, sf, quant_mult, supports_quant_int, ds_dir)\n",
    "    metrics_csv = ds_dir / f\"metrics_SF{sf}.csv\"\n",
    "    df.to_csv(metrics_csv, index=False)\n",
    "    print(f\"[metrics] saved {metrics_csv}\")\n",
    "\n",
    "    # Plot\n",
    "    plot_all(df, dataset, ds_dir / \"figures\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_retail.csv\"\n",
    "retail_sup = [25_000, 50_000, 60_000, 70_000, 80_000, 90_000, 100_000]\n",
    "\n",
    "run_pipeline(retail, sf=50, supports_quant_int=retail_sup, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_connect.csv\"\n",
    "connect_sup = [8_000_000, 8_500_000, 9_000_000, 9_500_000, 10_000_000]\n",
    "\n",
    "run_pipeline(connect, sf=25, supports_quant_int=connect_sup, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737eea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kosarak = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_kosarak.csv\"\n",
    "kosarak_sup = [250_000, 300_000, 350_000, 400_000, 450_000, 500_000, 550_000, 600_000, 650_000, 700_000]\n",
    "\n",
    "run_pipeline(kosarak, sf=10, supports_quant_int=kosarak_sup, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumsb = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_pumsb.csv\"\n",
    "pumsb_sup = [6_000_000, 5_500_000, 5_000_000, 4_500_000, 4_000_000]\n",
    "\n",
    "run_pipeline(pumsb, sf=25, supports_quant_int=pumsb_sup, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab04144",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumsb = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_T10I4D100K.csv\"\n",
    "pumsb_sup = [550_000, 500_000, 450_000, 400_000, 350_000, 300_000, 200_000]\n",
    "\n",
    "run_pipeline(pumsb, sf=50, supports_quant_int=pumsb_sup, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577f75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
