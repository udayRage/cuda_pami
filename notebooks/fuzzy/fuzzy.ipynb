{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111744c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported cuFFIMiner and naiveFFIMiner.\n",
      "Project Root: /export/home1/ltarun/cuda_pami\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This code adds the project's root directory to the Python path.\n",
    "# This is necessary so that both the 'src' and 'scripts' directories can be found.\n",
    "project_root = Path(os.getcwd()).parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Now we can import the miners and script functions using their full paths from the root\n",
    "from src.algorithms.fuzzy.cuFFIMiner import cuFFIMiner\n",
    "from src.algorithms.fuzzy.naiveFFIMiner import naiveFFIMiner\n",
    "\n",
    "print(\"Successfully imported cuFFIMiner and naiveFFIMiner.\")\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "data_dir = project_root / 'data' / 'fuzzy'\n",
    "results_dir = project_root / 'results' / 'fuzzy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31a97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, time, requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from scripts.replicate_file import replicate_file\n",
    "from scripts.fixedpoint_normalize import normalize_file\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.convert_to_parquet import convert_text_to_parquet\n",
    "\n",
    "# New naming convention and directory layout:\n",
    "#   data/fuzzy/<dataset_name>/ <dataset_name>.csv (original)\n",
    "#                                 <dataset_name>_SF{N}_floating.ext\n",
    "#                                 <dataset_name>_SF{N}_fixed.ext\n",
    "#                                 <dataset_name>_SF{N}_quant_mult.txt\n",
    "#                                 <dataset_name>_SF{N}_fixed.parquet\n",
    "#                                 <dataset_name>_SF{N}_floating.parquet (NEW)\n",
    "#   results/fuzzy/<dataset_name>/ ...\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers using existing scripts\n",
    "# -----------------------------\n",
    "\n",
    "def _dataset_filename_from_url(url: str) -> str:\n",
    "    return Path(url.split(\"?\")[0]).name  # e.g. Fuzzy_retail.csv\n",
    "\n",
    "def _dataset_name_no_ext(filename: str) -> str:\n",
    "    return Path(filename).stem  # e.g. Fuzzy_retail\n",
    "\n",
    "# We will place each dataset inside its own subfolder under data_dir\n",
    "\n",
    "def download_dataset(url: str, base_data_dir: Path) -> Path:\n",
    "    filename = _dataset_filename_from_url(url)\n",
    "    name_root = _dataset_name_no_ext(filename)\n",
    "    dataset_dir = base_data_dir / name_root\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local_path = dataset_dir / filename\n",
    "    if local_path.exists():\n",
    "        print(f\"[download] Existing: {local_path}\")\n",
    "        return local_path\n",
    "    # Fallback: if legacy path (without subfolder) exists, move it\n",
    "    legacy_path = base_data_dir / filename\n",
    "    if legacy_path.exists():\n",
    "        print(f\"[download] Moving legacy file into subfolder: {legacy_path} -> {local_path}\")\n",
    "        local_path.write_bytes(legacy_path.read_bytes())\n",
    "        return local_path\n",
    "    print(f\"[download] Fetch {url}\")\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    local_path.write_bytes(r.content)\n",
    "    print(f\"[download] Saved {local_path}\")\n",
    "    return local_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ensure_floating_sf(original: Path, sf: int) -> Path:\n",
    "    if sf < 1: sf = 1\n",
    "    float_path = original.with_name(f\"{original.stem}_SF{sf}_floating{original.suffix}\")\n",
    "    if float_path.exists():\n",
    "        print(f\"[SF] Using existing floating SF file: {float_path.name}\")\n",
    "        return float_path\n",
    "    replicate_file(str(original), sf, str(float_path))\n",
    "    return float_path\n",
    "\n",
    "def ensure_fixed_variant(floating_path: Path) -> Tuple[Path, int]:\n",
    "    stem = floating_path.stem.replace('_floating','')  # e.g. base_SF10\n",
    "    fixed_path = floating_path.with_name(f\"{stem}_fixed{floating_path.suffix}\")\n",
    "    quant_file = floating_path.with_name(f\"{stem}_quant_mult.txt\")\n",
    "    # Backward compatibility: if only old scale file exists, read it and rename\n",
    "    legacy_scale = floating_path.with_name(f\"{stem}_scale.txt\")\n",
    "    if fixed_path.exists() and quant_file.exists():\n",
    "        quant_val = int(quant_file.read_text().strip())\n",
    "        print(f\"[fixed] Reusing existing fixed file: {fixed_path.name} quant_mult={quant_val}\")\n",
    "        return fixed_path, quant_val\n",
    "    if fixed_path.exists() and legacy_scale.exists():\n",
    "        quant_val = int(legacy_scale.read_text().strip())\n",
    "        quant_file.write_text(str(quant_val)+'\\n')\n",
    "        print(f\"[fixed] Upgraded legacy scale -> quant_mult: {legacy_scale.name} -> {quant_file.name}\")\n",
    "        return fixed_path, quant_val\n",
    "    fixed_generated, quant_val = normalize_file(str(floating_path))\n",
    "    return Path(fixed_generated), quant_val\n",
    "\n",
    "def ensure_fixed_parquet(fixed_text: Path) -> Path:\n",
    "    parquet_path = fixed_text.with_suffix('.parquet')\n",
    "    if parquet_path.exists():\n",
    "        print(f\"[parquet-fixed] Existing: {parquet_path.name}\")\n",
    "        return parquet_path\n",
    "    convert_text_to_parquet(str(fixed_text), str(parquet_path))\n",
    "    return parquet_path\n",
    "\n",
    "def ensure_floating_parquet(floating_text: Path) -> Path:\n",
    "    \"\"\"Create a floating parquet preserving float probabilities for naiveFFIMiner.\n",
    "    Schema: item:str, prob:float64, txn_id:uint32\n",
    "    Naming: *_SF{N}_floating.parquet\n",
    "    \"\"\"\n",
    "    parquet_path = floating_text.with_suffix('.parquet')\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            sample = pd.read_parquet(parquet_path).head(5)\n",
    "            # If any prob has a fractional component assume it's already floating\n",
    "            if not sample.empty and sample['prob'].dtype.kind in {'f'} and any((sample['prob'] % 1) != 0):\n",
    "                print(f\"[parquet-floating] Existing floating parquet: {parquet_path.name}\")\n",
    "                return parquet_path\n",
    "            else:\n",
    "                print(f\"[parquet-floating] Existing parquet appears integer or non-float; rebuilding to preserve floats.\")\n",
    "        except Exception:\n",
    "            print(\"[parquet-floating] Could not validate existing parquet; rebuilding.\")\n",
    "    import re as _re\n",
    "    df = pd.read_csv(\n",
    "        floating_text,\n",
    "        sep=\":\",\n",
    "        header=None,\n",
    "        names=[\"items_str\", \"values_str\"],\n",
    "        dtype=str,\n",
    "        engine='python'\n",
    "    )\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    pattern = r\"[\\t\\r\\n ]+$\"\n",
    "    df[\"items_str\"] = df[\"items_str\"].str.replace(pattern, \"\", regex=True)\n",
    "    df[\"values_str\"] = df[\"values_str\"].str.replace(pattern, \"\", regex=True)\n",
    "    df[\"items\"] = df[\"items_str\"].str.split(\"\\t\")\n",
    "    df[\"values\"] = df[\"values_str\"].str.split(\"\\t\")\n",
    "    records = []\n",
    "    for txn_id, (items, values) in enumerate(zip(df[\"items\"], df[\"values\"]), start=1):\n",
    "        if len(items) != len(values):\n",
    "            continue\n",
    "        for it, val in zip(items, values):\n",
    "            if not it:\n",
    "                continue\n",
    "            try:\n",
    "                fval = float(val)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            records.append((it, fval, txn_id))\n",
    "    # Keep probability as float64 for arithmetic; do NOT coerce to string (previous bug)\n",
    "    out_df = pd.DataFrame(records, columns=[\"item\",\"prob\",\"txn_id\"]).astype({\"item\":\"string\",\"prob\":\"float64\",\"txn_id\":\"uint32\"})\n",
    "    out_df.to_parquet(parquet_path, engine=\"pyarrow\", index=False)\n",
    "\n",
    "    print(f\"[parquet-floating] Wrote {len(out_df)} rows -> {parquet_path.name}\")\n",
    "    return parquet_path\n",
    "\n",
    "# Mapping supports\n",
    "\n",
    "def support_to_float(support_int: int, quant_mult: int) -> float:\n",
    "    if quant_mult > 0:\n",
    "        return support_int / quant_mult\n",
    "    return float(support_int)\n",
    "\n",
    "# Mining both miners with unified quant_mult\n",
    "\n",
    "def run_both_miners(fixed_parquet: Path, floating_parquet: Path, quant_mult: int, supports_scaled: List[int], results_subdir: Path, memory_type: str = \"global\", debug: bool = False) -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    results_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for sup_int in supports_scaled:\n",
    "        cuffi_patterns_path = results_subdir / f\"patterns_cuffi_sup{sup_int}.txt\"\n",
    "        if cuffi_patterns_path.exists():\n",
    "            print(f\"[cuFFIMiner] Skip sup={sup_int} (exists)\")\n",
    "        else:\n",
    "            try:\n",
    "                algo_c = cuFFIMiner(str(fixed_parquet), min_support=sup_int, scaling_factor=quant_mult, memory_type=memory_type, debug=debug)\n",
    "                algo_c.mine(); algo_c.save(cuffi_patterns_path); algo_c.print_results()\n",
    "                rows.append({\"algorithm\":\"cuFFIMiner\",\"support_quant_int\":sup_int,\"quant_mult\":quant_mult,\"exec_time\":algo_c.get_execution_time(),\"cpu_mem_mb\":algo_c.get_memory_usage(),\"gpu_mem_bytes\":getattr(algo_c,'_gpu_memory_usage',None),\"patterns_found\":algo_c.get_pattern_count()})\n",
    "            except Exception as e:\n",
    "                print(f\"[cuFFIMiner][ERROR] sup={sup_int}: {e}\")\n",
    "                rows.append({\"algorithm\":\"cuFFIMiner\",\"support_quant_int\":sup_int,\"quant_mult\":quant_mult,\"error\":str(e)})\n",
    "\n",
    "        sup_float = support_to_float(sup_int, quant_mult)\n",
    "        print('sup_float:', sup_float)\n",
    "        print(f\"[naiveFFIMiner] quant_int={sup_int} -> float={sup_float} (forced quant_mult={quant_mult})\")\n",
    "        naive_patterns_path = results_subdir / f\"patterns_naive_sup{sup_int}.txt\"\n",
    "        if naive_patterns_path.exists():\n",
    "            print(f\"[naiveFFIMiner] Skip sup={sup_int} (exists)\")\n",
    "        else:\n",
    "            try:\n",
    "                algo_n = naiveFFIMiner(str(floating_parquet), min_support=sup_float, quant_mult=quant_mult, debug=debug)\n",
    "                algo_n.mine(); algo_n.save(naive_patterns_path); algo_n.print_results()\n",
    "                rows.append({\"algorithm\":\"naiveFFIMiner\",\"support_quant_int\":sup_int,\"support_float\":sup_float,\"quant_mult\":quant_mult,\"exec_time\":algo_n.get_execution_time(),\"cpu_mem_mb\":algo_n.get_memory_usage(),\"gpu_mem_bytes\":getattr(algo_n,'_gpu_memory_usage',None),\"patterns_found\":algo_n.get_pattern_count()})\n",
    "            except Exception as e:\n",
    "                print(f\"[naiveFFIMiner][ERROR] sup={sup_int}: {e}\")\n",
    "                rows.append({\"algorithm\":\"naiveFFIMiner\",\"support_quant_int\":sup_int,\"support_float\":sup_float,\"quant_mult\":quant_mult,\"error\":str(e)})\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd475f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams.update({\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    'figure.dpi': 150,\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'legend.fontsize': 9,\n",
    "})\n",
    "\n",
    "_METRIC_LABELS = {\n",
    "    'exec_time': 'Execution Time (s)',\n",
    "    'cpu_mem_mb': 'Peak CPU Memory (MB)',\n",
    "    'gpu_mem_bytes': 'GPU Memory (MB)',\n",
    "    'patterns_found': 'Patterns Found',\n",
    "}\n",
    "\n",
    "DEFAULT_FIG_CFG = {\n",
    "    'width': 5.0,\n",
    "    'height': 3.0,\n",
    "    'legend_loc': 'best',\n",
    "    'tight_layout': True,\n",
    "}\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    if 'gpu_mem_bytes' in d.columns:\n",
    "        d['gpu_mem_mb'] = d['gpu_mem_bytes'] / (1024**2)\n",
    "    return d\n",
    "\n",
    "def plot_metric(df: pd.DataFrame, metric: str, dataset_name: str, output_dir: Path,\n",
    "                fig_cfg: dict | None = None, scale_x: bool = False):\n",
    "    cfg = {**DEFAULT_FIG_CFG, **(fig_cfg or {})}\n",
    "    d = _prep_df(df)\n",
    "    if metric == 'gpu_mem_mb' and 'gpu_mem_mb' not in d.columns:\n",
    "        print('Skipping gpu_mem_mb (not present)')\n",
    "        return\n",
    "    xcol = 'support_quant_int'\n",
    "    if scale_x and 'quant_mult' in d.columns:\n",
    "        x = d[xcol] * d['quant_mult']\n",
    "        xlabel = 'Support Threshold (raw * quant_mult)'\n",
    "    else:\n",
    "        x = d[xcol]\n",
    "        xlabel = 'Support Threshold (quantized int)'\n",
    "    fig, ax = plt.subplots(figsize=(cfg['width'], cfg['height']))\n",
    "    for algo, sub in d.groupby('algorithm'):\n",
    "        ycol = metric if metric != 'gpu_mem_mb' else 'gpu_mem_mb'\n",
    "        ax.plot(x.loc[sub.index], sub[ycol], marker='o', label=algo)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(_METRIC_LABELS.get(metric, metric))\n",
    "    ax.set_title(f\"{dataset_name} â€“ {_METRIC_LABELS.get(metric, metric)}\")\n",
    "    ax.grid(alpha=0.25, linestyle=':')\n",
    "    ax.legend(loc=cfg['legend_loc'])\n",
    "    if cfg.get('tight_layout', True):\n",
    "        fig.tight_layout()\n",
    "    _ensure_dir(output_dir)\n",
    "    out_file = output_dir / f\"{dataset_name}_{metric}.pdf\"\n",
    "    fig.savefig(out_file, format='pdf')\n",
    "    plt.close(fig)\n",
    "    print(f\"[figure] Wrote {out_file}\")\n",
    "\n",
    "def generate_all_figures(dataset_name: str, metrics_df: pd.DataFrame | None = None,\n",
    "                          metrics_path: Path | None = None, output_subdir: str = 'figures',\n",
    "                          metrics: list[str] | None = None, fig_cfg: dict | None = None,\n",
    "                          scale_x: bool = False):\n",
    "    if metrics_df is None:\n",
    "        if metrics_path:\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "        else:\n",
    "            ds_dir = results_dir / dataset_name\n",
    "            files = sorted(ds_dir.glob('metrics_SF*.csv'))\n",
    "            if not files:\n",
    "                raise FileNotFoundError(f\"No metrics file found in {ds_dir}\")\n",
    "            dfs = [pd.read_csv(f) for f in files]\n",
    "            metrics_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "    if metrics is None:\n",
    "        metrics = ['exec_time','cpu_mem_mb','gpu_mem_mb','patterns_found']\n",
    "    out_dir = results_dir / dataset_name / output_subdir\n",
    "    for m in metrics:\n",
    "        if m not in metrics_df.columns and not (m == 'gpu_mem_mb' and 'gpu_mem_bytes' in metrics_df.columns):\n",
    "            print(f\"[figure] Skip missing metric: {m}\")\n",
    "            continue\n",
    "        plot_metric(metrics_df, m, dataset_name, out_dir, fig_cfg=fig_cfg, scale_x=scale_x)\n",
    "    print(\"[figure] All requested figures generated.\")\n",
    "\n",
    "def run_experiment(dataset_url: str, sf: int, supports_quant_int: List[int], memory_type: str = 'global', debug: bool = False,\n",
    "                   generate_figures: bool = True, fig_metrics: List[str] | None = None, fig_cfg: Dict[str, Any] | None = None,\n",
    "                   scale_x: bool = False, fig_subdir: str = 'figures') -> pd.DataFrame:\n",
    "    print(\"========== RUN EXPERIMENT (SF + quant_mult unified) ==========\")\n",
    "    print(f\"Dataset URL : {dataset_url}\")\n",
    "    print(f\"SF (concat) : {sf}\")\n",
    "    print(f\"Quantized integer supports: {supports_quant_int}\")\n",
    "\n",
    "    original = download_dataset(dataset_url, data_dir)\n",
    "    dataset_name = original.parent.name\n",
    "    floating_sf_text = ensure_floating_sf(original, sf)\n",
    "    fixed_file_text, quant_mult = ensure_fixed_variant(floating_sf_text)\n",
    "\n",
    "    fixed_parquet = ensure_fixed_parquet(fixed_file_text)\n",
    "    floating_parquet = ensure_floating_parquet(floating_sf_text)\n",
    "\n",
    "    result_dir = results_dir / dataset_name\n",
    "    metrics_df = run_both_miners(fixed_parquet, floating_parquet, quant_mult, supports_quant_int, result_dir, memory_type=memory_type, debug=debug)\n",
    "\n",
    "    metrics_file = result_dir / f\"metrics_SF{sf}.csv\"\n",
    "    if not metrics_file.exists() or len(metrics_df) > 0:\n",
    "        metrics_df.to_csv(metrics_file, index=False)\n",
    "        print(f\"[metrics] Saved {metrics_file}\")\n",
    "\n",
    "    if generate_figures:\n",
    "        try:\n",
    "            generate_all_figures(dataset_name, metrics_df=metrics_df, metrics=fig_metrics, fig_cfg=fig_cfg, scale_x=scale_x, output_subdir=fig_subdir)\n",
    "        except Exception as e:\n",
    "            print(f\"[figure][ERROR] {e}\")\n",
    "\n",
    "    print(\"============ DONE ============\")\n",
    "    return metrics_df\n",
    "\n",
    "run_complete_experiment = run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fb90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RUN EXPERIMENT (SF + quant_mult unified) ==========\n",
      "Dataset URL : https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_retail.csv\n",
      "SF (concat) : 100\n",
      "Quantized integer supports: [40000, 45000, 50000, 55000, 60000]\n",
      "[download] Existing: /export/home1/ltarun/cuda_pami/data/fuzzy/Fuzzy_retail/Fuzzy_retail.csv\n",
      "[SF] Using existing floating SF file: Fuzzy_retail_SF100_floating.csv\n",
      "[fixed] Reusing existing fixed file: Fuzzy_retail_SF100_fixed.csv quant_mult=10\n",
      "[parquet-fixed] Existing: Fuzzy_retail_SF100_fixed.parquet\n",
      "[parquet-floating] Existing floating parquet: Fuzzy_retail_SF100_floating.parquet\n",
      "[cuFFIMiner] Skip sup=40000 (exists)\n",
      "sup_float: 4000.0\n",
      "[naiveFFIMiner] quant_int=40000 -> float=4000.0 (forced quant_mult=10)\n",
      "[naiveFFIMiner] Skip sup=40000 (exists)\n",
      "[cuFFIMiner] Skip sup=45000 (exists)\n",
      "sup_float: 4500.0\n",
      "[naiveFFIMiner] quant_int=45000 -> float=4500.0 (forced quant_mult=10)\n",
      "[naiveFFIMiner] Skip sup=45000 (exists)\n",
      "[cuFFIMiner] Skip sup=50000 (exists)\n",
      "sup_float: 5000.0\n",
      "[naiveFFIMiner] quant_int=50000 -> float=5000.0 (forced quant_mult=10)\n",
      "[naiveFFIMiner] Skip sup=50000 (exists)\n",
      "[cuFFIMiner] Skip sup=55000 (exists)\n",
      "sup_float: 5500.0\n",
      "[naiveFFIMiner] quant_int=55000 -> float=5500.0 (forced quant_mult=10)\n",
      "[naiveFFIMiner] Skip sup=55000 (exists)\n",
      "[cuFFIMiner] Skip sup=60000 (exists)\n",
      "sup_float: 6000.0\n",
      "[naiveFFIMiner] quant_int=60000 -> float=6000.0 (forced quant_mult=10)\n",
      "[naiveFFIMiner] Skip sup=60000 (exists)\n",
      "[figure] Skip missing metric: exec_time\n",
      "[figure] Skip missing metric: cpu_mem_mb\n",
      "[figure] Skip missing metric: gpu_mem_mb\n",
      "[figure] Skip missing metric: patterns_found\n",
      "[figure] All requested figures generated.\n",
      "============ DONE ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_retail.csv\"\n",
    "retail_sup = [40000, 45000, 50000, 55000, 60000]\n",
    "\n",
    "metrics_retail = run_experiment(retail, 100, retail_sup)\n",
    "metrics_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfb2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RUN EXPERIMENT (SF + quant_mult unified) ==========\n",
      "Dataset URL : https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_connect.csv\n",
      "SF (concat) : 100\n",
      "Quantized integer supports: [25000000]\n",
      "[download] Existing: /export/home1/ltarun/cuda_pami/data/fuzzy/Fuzzy_connect/Fuzzy_connect.csv\n",
      "[SF] Using existing floating SF file: Fuzzy_connect_SF100_floating.csv\n",
      "[fixed] Reusing existing fixed file: Fuzzy_connect_SF100_fixed.csv quant_mult=10\n",
      "[parquet-fixed] Existing: Fuzzy_connect_SF100_fixed.parquet\n",
      "[parquet-floating] Existing floating parquet: Fuzzy_connect_SF100_floating.parquet\n",
      "\n",
      "--- cuFFIMiner Results ---\n",
      "Execution Time: 50.6945 seconds\n",
      "Peak CPU Memory Usage: 38185.53 MB\n",
      "Peak GPU Memory Usage: 19409.06 MB\n",
      "Patterns Found: 5004\n",
      "------------------------------\n",
      "sup_float: 2500000.0\n",
      "[naiveFFIMiner] quant_int=25000000 -> float=2500000.0 (forced quant_mult=10)\n",
      "\n",
      "--- naiveFFIMiner Results ---\n",
      "Execution Time: 55.6094 seconds\n",
      "Peak CPU Memory Usage: 38185.53 MB\n",
      "Peak GPU Memory Usage: 19027.06 MB\n",
      "Patterns Found: 5004\n",
      "---------------------------------\n",
      "[metrics] Saved /export/home1/ltarun/cuda_pami/results/fuzzy/Fuzzy_connect/metrics_SF100.csv\n",
      "[figure] Wrote /export/home1/ltarun/cuda_pami/results/fuzzy/Fuzzy_connect/figures/Fuzzy_connect_exec_time.pdf\n",
      "[figure] Wrote /export/home1/ltarun/cuda_pami/results/fuzzy/Fuzzy_connect/figures/Fuzzy_connect_cpu_mem_mb.pdf\n",
      "[figure] Wrote /export/home1/ltarun/cuda_pami/results/fuzzy/Fuzzy_connect/figures/Fuzzy_connect_gpu_mem_mb.pdf\n",
      "[figure] Wrote /export/home1/ltarun/cuda_pami/results/fuzzy/Fuzzy_connect/figures/Fuzzy_connect_patterns_found.pdf\n",
      "[figure] All requested figures generated.\n",
      "============ DONE ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>support_quant_int</th>\n",
       "      <th>quant_mult</th>\n",
       "      <th>exec_time</th>\n",
       "      <th>cpu_mem_mb</th>\n",
       "      <th>gpu_mem_bytes</th>\n",
       "      <th>patterns_found</th>\n",
       "      <th>support_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cuFFIMiner</td>\n",
       "      <td>25000000</td>\n",
       "      <td>10</td>\n",
       "      <td>50.694450</td>\n",
       "      <td>38185.527344</td>\n",
       "      <td>20351877120</td>\n",
       "      <td>5004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naiveFFIMiner</td>\n",
       "      <td>25000000</td>\n",
       "      <td>10</td>\n",
       "      <td>55.609369</td>\n",
       "      <td>38185.527344</td>\n",
       "      <td>19951321088</td>\n",
       "      <td>5004</td>\n",
       "      <td>2500000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       algorithm  support_quant_int  quant_mult  exec_time    cpu_mem_mb  \\\n",
       "0     cuFFIMiner           25000000          10  50.694450  38185.527344   \n",
       "1  naiveFFIMiner           25000000          10  55.609369  38185.527344   \n",
       "\n",
       "   gpu_mem_bytes  patterns_found  support_float  \n",
       "0    20351877120            5004            NaN  \n",
       "1    19951321088            5004      2500000.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_connect.csv\"\n",
    "connect_sup = [25000000, 24000000, 23000000, 22000000, 21000000]\n",
    "\n",
    "metrics_connect = run_experiment(connect, 100, connect_sup)\n",
    "metrics_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737eea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RUN EXPERIMENT (SF + quant_mult unified) ==========\n",
      "Dataset URL : https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_kosarak.csv\n",
      "SF (concat) : 100\n",
      "Quantized integer supports: [900000, 1000000]\n",
      "[download] Fetch https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_kosarak.csv\n",
      "[download] Saved /export/home1/ltarun/cuda_pami/data/fuzzy/Fuzzy_kosarak/Fuzzy_kosarak.csv\n",
      "[replicate] Wrote: /export/home1/ltarun/cuda_pami/data/fuzzy/Fuzzy_kosarak/Fuzzy_kosarak_SF100_floating.csv (SF=100)\n"
     ]
    }
   ],
   "source": [
    "kosarak = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_kosarak.csv\"\n",
    "kosarak_sup = [900000, 1000000]\n",
    "\n",
    "metrics_kosarak = run_experiment(kosarak, 100, kosarak_sup)\n",
    "metrics_kosarak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumsb = \"https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_pumsb.csv\"\n",
    "pumsb_sup = [100000, 200000]\n",
    "\n",
    "metrics_pumsb = run_experiment(pumsb, 100, pumsb_sup)\n",
    "metrics_pumsb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
